\documentclass{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{graphics, graphicx}
\usepackage{subcaption}
\usepackage[export]{adjustbox}[2011/08/13]
\usepackage{xcolor}


\usepackage{calc}
\captionsetup{compatibility=false}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\softmax}{SoftMax}


\title{Детекция эмоций. Сравнение и анализ классических методов машинного обучения и методов обучения с трансформерами}

\author{ Панин Никита Александрович \\
        Факультет вычислительной математики и кибернетики \\
        МГУ им. Ломоносова \\
        \texttt{s02200456@gse.cs.msu.ru} \\
	%% examples of more authors
	\And
	д.ф-м.н., профессор, Воронцов Константин Вячеславович \\
        Факультет вычислительной математики и кибернетики \\
        МГУ им. Ломоносова \\
        \texttt{vokov@forecsys.ru} \\
	%% \AND
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

\date{}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={Детекция эмоций. Сравнение и анализ классических методов машинного обучения и методов обучения с трансформерами},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={Воронцов Константин Вячеславович,Панин Никита Александрович },
pdfkeywords={},
}

\begin{document}
\maketitle

\begin{abstract}
	В работе рассмотрена задача детекции эмоций на датасете, в основу которого вошел WASSA датасет из твитов для детекции эмоций. На выходе алгоритма классификации эмоций в твитах была одна из 5 эмоций: нейтральная эмоция, грусть, страх, радость, гнев. Были применены различные методы "классического" машинного обучения, такие как, SVM, логистическая регрессия, метод k-ближайших соседей и наивный байесовский классификатор. Также классификация эмоций была проведена с помощью файн-тюнинга нескольких версий BERT. Основной целью работы являлось проведение сравнительного анализа для классических моделей машинного обучения(wKNN, Multinomial Bayes Classifier, Logistic Regression, SVM) и для моделей глубокого обучения (в качестве предобученной модели брались BERT, RoBERTa, BERTweet и их large-версии). В результате исследования было показано, что по метрике accuracy для моделей классического обучения c tf-idf векторизацией текстов лучше всего работает SVM с RBF ядром ($accuracy \approx 0.8387$ на тесте), а наиболее качественные результаты получаются с помощью предложенной в исследовании модели с предобученным BERTweet ($accuracy \approx 0.88$ на тесте).
\end{abstract}


\keywords{Детекция эмоций \and  NLP}

\section{Введение}
 Детекция эмоций в текстах стала актуальным направлением исследований в области обработки естественного языка (NLP) и анализа тональности и привлекает все большее внимание в последние два десятилетия~\cite{affectdetectionintexts, jiawenfuji}. Термины "распознавание эмоций"({\itshape{emotion detection}}) и  "анализ тональности"({\itshape{sentiment analysis}}) часто используются как взаимозаменяемые, хотя между этими двумя понятиями существуют очевидные различия~\cite{yadollahi}. Анализ тональности в основном измеряет субъективное отношение с точки зрения полярности настроения: нейтральное, положительное, негативное. Выявление эмоций предполагает идентификацию более детальных эмоциональных состояний, например,  счастье, гнев, страх, удивление. 

Эмоции имеют множество применений в разных сферах.  В маркетинге анализ предпочтений потребителей помогает улучшить бизнес-стратегии~\cite{cambria}. В социальных сетях распознавание агрессивных эмоций  помогает выявить потенциальных преступников или террористов~\cite{cheong}. Мониторинг эмоций в реальном времени на основе данных социальных сетей может помочь в профилактике самоубийств~\cite{ren}. Определение эмоций во время кризисов или катастроф позволяет понять чувства людей по отношению к конкретной ситуации, что способствует управлению в кризис и принятию важных решений~\cite{ahmad}. 




\subsection{Существующие решения}


Ванг и др.~\cite{wang} создали большой набор данных твитов, используя хэштеги эмоций. Они применили два различных классификатора, логистическую регрессию и Naïve Bayes, чтобы исследовать эффективность различных признаков, таких как n-граммы, лексикон эмоций и информация о части речи, для задачи идентификации эмоций. Наибольшая достигнутая точность составила 0,6557.


Мохаммад~\cite{mohammad} создал корпус твитов, маркированных эмоциями, используя хэштеги. Он применил бинарные SVM, по одному для каждой из шести основных эмоций Экмана~\cite{ekman}, и использовал наличие или отсутствие униграмм и биграмм в качестве бинарных признаков. Бинарные классификаторы смогли предсказать эмоции со сбалансированным F1-score 0,499.


Янссенс и др.~\cite{janssens} исследовали влияние использования слабых меток по сравнению с сильными метками на распознавание эмоций для корпуса, состоящего из 341 931 твита. Слабые метки были созданы путем использования хэштегов твитов, а сильные метки - с помощью краудсорсинга. Характеристики, извлеченные путем объединения n-грамм и TF-IDF (Term Frequency-Inverse Document Frequency), были применены к пяти алгоритмам классификации: Стохастический градиентный спуск, SVM, Naïve Bayes, Nearest Centroid и Ridge. Результаты показали снижение F1-score на 9,25\% при использовании слабых меток.

К сожалению, классические методы машинного обучения не могут учесть последовательную природу текста, поэтому некоторые модели глубокого обучения, такие как рекуррентные нейронные сети (RNN), LSTM~\cite{hochreiter1} и GRU~\cite{cho}, стали более перспективными в определении эмоций в тексте~\cite{kratzwald, chatterjee, xu}. Хотя рекуррентные модели принимают во внимание последовательный характер текста и показывают передовые результаты для различных задач NLP, они обладают некоторыми слабостями: медленная скорость, необходимость обучения с нуля и ограниченная способность улавливать долгосрочные зависимости в тексте~\cite{hochreiter2}. Также требуется большой объем размеченных данных для обучения. Подготовка большого объема размеченных данных является трудоемкой и дорогостоящей процедурой, и именно здесь вступает в игру перенос обучения(transfer learning). С его помощью можно добиться лучших результатов по сравнению с традиционными моделями глубокого обучения с гораздо меньшим количеством обучающей выборки. Предварительно обученные языковые модели, такие как BERT~\cite{devlin} (Bidirectional Encoder Representations from Transformers) и его варианты, OpenAI GPT (Generative Pre-trained Transformer)~\cite{radford} и Transformer-XL~\cite{dai}, получили широкое распространение в различных задачах NLP и продемонстрировали впечатляющие результаты.

 Некоторые работы используют предварительно обученные языковые модели для классификации эмоций или тональности в тексте. Например, исследование~\cite{prottasha} использует BERT в качестве слоя эмбеддинга, после которого выводы передаются через слои CNN и BiLSTM для анализа настроений на бенгальском языке.

 \section{Постановка задачи}
 В  данной работе была рассмотрена задача детекции эмоций на датасете, в основу которого вошел WASSA датасет из твитов для детекции эмоций~\cite{wassa}. Были применены различные методы "классического" \ машинного обучения, такие как, SVM, логистическая регрессия, метод k-ближайших
 соседей и наивный байесовский классификатор. Также классификация эмоций была проведена с помощью файн-тюнинга нескольких версий BERT. 

 Формально каждый твит $d_i$ векторизовался в вектор $x_i \in \mathbb{R}^h$, описывающий структуру $d_i$, где $h$ - размерность вектора $x_i$. Далее полученный эмбеддинг
 $x_i$ поступал на вход одному из перечисленных выше алгоритмов классификации $\mathcal{A}$ и на выходе $\mathcal{A}(i)$ получали вероятности эмоций $p_{ik},\ \sum_{k=1}^5p_{ik} = 1$. Итоговым ответом алгоритма была эмоция, соответсвующая наибольшей вероятности. В качестве эмоций рассматривалось  5 типов: нейтральная эмоция, грусть, страх, радость, гнев.

 За основу бралась статья~\cite{albu}. Основной целью работы являлось проведение сравнительного анализа полученных моделей. Также в работе предлагается сравнение новых моделей, таких как large-версии бертов, логистическая регрессия, wKNN, SVM с другими ядрами, с результатами из основной статьи.

\section{Условия экспериментов}
\subsection{Описание данных}
Данные\footnote{Данные были взяты с: \\ https://github.com/alexalbu98/Emotion-Detection-From-Tweets-Using-BERT-and-SVM-Ensemble-Model/blob/master/dataset.zip}, используемые для обучения и оценки эмоций в твитах, были частично получены из набора данных WASSA, представленного участникам на семинаре по компьютерным методам анализа субъективности, настроения и социальных сетей (WASSA-2017)~\cite{wassa}. Было выбрано по 1500 твитов для каждой из четырех эмоций: страх, грусть, радость и гнев, при этом информация об интесивности эмоций из датасета была исключена. К этим четырем категориям был добавлен еще один класс из 1500 нейтральных твитов, так как в исследовании\cite{koppel} было показано, что наличие нейтрального класса важно для классификации, поскольку это позволяет модели не относить неизвестные эмоции к одному из изучаемых классов. Нейтральные твиты были взяты с CrowdFlower. 
Данные были разделены таким образом, чтобы обеспечить хороший баланс между классами~\cite{borovicka}. Сбалансированный набор данных содержит одинаковое количество примеров для каждого класса, что гарантирует, что модель не будет уделять больше внимания крупным классам при классификации.
Для обучения бралось 80\% данных, для теста и валидации по 10\%.

\subsection{Метрики}
Поскольку используемый датасет сбалансирован по классам, то в качестве основной метрики была взята доля правильных классификаций(accuracy), которая плохо работает в случае дисбаланса классов. Также сравнения велись по точности(Precision), полноте(Recall), F1 мере(F1) с макроусреднением. 

Для каждого класса $y \in Y$:
\begin{itemize}
    \item $TP_y$ -- верные положительные
    \item $FP_y$ -- ложные положительные
    \item $FN_y$ -- ложные отрицательные
\end{itemize}
Точность, полнота и F1 мера с {\bfseries макроусреднением}:
            $$ 
                Precision_{macro} = \frac{1}{|Y|}\sum_{y}\frac{TP_y}{TP_y + FP_y}
            $$
            $$ 
                Recall_{macro} = \frac{1}{|Y|}\sum_{y}\frac{TP_y}{TP_y + FN_y}
            $$
            $$
                F1_{macro} = \frac{2 * Precision_{macro} * Recall_{macro}}
                                  {Precision_{macro} + Recall_{macro}}
            $$
В дальнейшем изложении пометка "macro"\ будет опускаться.
\subsection{Классическое машинное обучение}
\subsubsection{Предобработка текстов}
"Сырые"\ твиты в большинстве своем это "грязные"{}, сильно зашумленные данные. Это сопряжено прежде всего с природой твитов: люди часто пишут сокращениями, с ошибками и прочее. При предобработке текстов использовались стандартные методы в машинном обучении для подготовки текстовых данных на вход алгоритму:
\begin{itemize}
    \item {\bfseries Эмотикон}\\
    Поскольку через текст часто сложно передать эмоции, эмотиконы приобрели очень большую популярность и пользователи нередко используют их в своих твитах. В связи с этим эмотиконы были переведены в текстовый формат с помощью библиотеки Demoji Python, чтобы они помогали в идентификации эмоций в твитах~\cite{wolny}.
    \item {\bfseries Хэштег}\\
    Хэштеги - это фразы без пробелов с префиксом в виде символа решетки (\#), которые часто используются пользователями для упоминания трендовой темы в Твиттере. Все хэштеги были заменены словами без символа решетки. Например, \#hello заменяется на hello. 
    \item{\bfseries Упоминание пользователей}\\
    У каждого пользователя Твиттера есть имя пользователя, связанное с ним. Пользователи часто упоминают других пользователей в своих твитах через @username. В текстах все упоминания пользователей убирались.
    \item{\bfseries URL}\\
    Пользователи часто делятся гиперссылками на другие веб-страницы в своих твитах. Какой-либо конкретный URL-адрес не важен для классификации текста и , если бы ссылки были оставлены, то это привело бы к очень разреженным признакам для текстов причем напрасно. Именно поэтому 
все URL-адреса в твитах были удалены.
    \item{\bfseries Фильтрация стоп-слов и стемминг}\\
    Эмпирически доказано, что фильтрация стоп-слов в наборе данных повышает производительность и скорость вычислений~\cite{saif}, поэтому стоп-слова удалялись. Стемминг также использовался и это еще один метод повышения производительности модели путем сведения производных слов к грамматическому корню, называемому "stem".
\end{itemize}

\subsubsection{Векторизация текстов}

После предобработки тексты векторизовались с помощью наиболее популярного векторного представления -- TF-IDF. Пусть $d$ - документ из коллекции документов $D$, $w_i$ - $i$-е слово из словаря $W$. Мощность (количество элементов) для множеств , как это принято в математике, будем обозначать $|\cdot|$. Тогда векторное представление каждого документа будет выглядеть так:
            $$
            d_{vec} = \begin{pmatrix}
                    tf\!-\!idf( w_1, d, D) \\
                    tf\!-\!idf( w_2, d, D) \\
                    \vdots\\
                    tf\!-\!idf({w_{|W|}}, d, D) \\
                \end{pmatrix},
            $$

            
где $$tf\!-\!idf(w_i, d, D) = \underbrace{tf(w_i, d)}_{term\ frequency} * 
                             \underbrace{idf(w_i, D)}_{inverse\ document\ frequency} =\\$$
                             $$
                             =\underbrace{\frac{\sum_{w' \in d}[w_i = w']\footnotemark}{\sum_{w' \in d}1}}_{tf(w_i, d)} *\ 
                             \underbrace{log\left(\frac{|D|}{\sum_{d \in D}[w_i \in d]}\right)}_{idf(w_i, D)}$$
                             \footnotetext{Используется нотация Айверсона}
Такие веса используются из предположения, что, чем больше документов, в которых встречается слово, тем меньше смысла оно несет в себе и следовательно его вес меньше.
\subsubsection{Модели}
\begin{enumerate}
    \item{\bfseries wKNN} \\
    Вместо классического KNN использовался взвешенный KNN (weighted KNN, wKNN)~\cite{dudani}, в котором веса не равномерно распределены для ближайших соседей объекта, а обратно пропорционально от расстояний от объекта до ближайших соседей. Основными параметрами для этого метрического метода являются: расстояние между текстами и $k$ ближайших соседей.

    В качестве метрики в метрических алгоритмах берут функции убывающие от "близости"{}, поэтому будем отталкиваться именно от "близости"{} текстов. Cогласно Хуанг~\cite{huang}, которая сравнивала различные меры близости для кластеризации текстов, лучшая оказалась косинусная мера:
    $$
    cosine\_similarity(x, y) = \frac{x^{T}y}{\|x\| \|y\|},
    $$ где x, y - векторные представления текстов, а $\|\cdot\|$ - норма вектора. В качестве косинусного расстояния бралась функция:
    $$
    cosine\_distance(x, y) = 1 - cosine\_similarity(x, y)
    $$
    При подборе $k$ можно было воспользоваться эмпирическим правилом и брать $k = \frac{\sqrt{|D|}}{2} = 38$(в нашем случае), однако, поскольку это лишь эмпирическое правило, было решено попробовать перебрать $k$ и оказалось, что лучшим выбором было $k = 52$
    
    \item{\bfseries Logistic Regression} \\
    Логистическая регрессия - это алгоритм машинного обучения с учителем, используемый для задач бинарной или многоклассовой классификации. В контексте машинного обучения он используется для прогнозирования вероятности принадлежности объекта к определенному классу. Существует несколько подходов для использования logistic regression: либо использовать множество бинарных логистических классификаторов и на основе их выдавать ответ (one-vs-one или one-vs-rest) или изменить лосс функцию и на выходе вместо сигмоиды для предсказания вероятностей использовать софтмакс. В данной работе использовался второй подход. Формализуем задачу многоклассовой логистической регрессии. \\
    
     Линейный классификатор при произвольном числе классов $|Y|$   
        $$
        a(x) = \argmax_{y \in Y}\langle w_y, x\rangle,\ x, w_y \in \mathbb{R},
        $$
        где $\langle \cdot, \cdot \rangle$ - скалярное произведение, $w_y$ - веса для метки $y \in Y$.\\
    Вероятность того, что объект $x$ относится к классу $y$:
        $$
        P(y|x, w) = \frac{exp(\langle w_y, x\rangle)}{\sum_{z \in Y}(\langle w_z, x\rangle)} = 
        \softmax_{y \in Y}(\langle w_y, x \rangle)
        $$
    Обучение состоит в максимизации правдоподобия (log-loss) с регулизацией:
    $$
        L(w) = \sum_{i=1}^{l}log(P(y_i|x_i, w) - \frac{\tau}{2}\sum_{y \in Y}\|w_y\|^2 \rightarrow \max_w
    $$
    Для $\tau$ было подобрано значение равное 1.

    \item{\bfseries Multinomial Naive Bayes}\\
    Мультиномиальный байесовский классификатор (Multinomial Naive Bayes Classifier) является разновидностью наивного байесовского классификатора, который использует мультиномиальное распределение для моделирования данных. Этот алгоритм особенно хорошо подходит для решения задач классификации текста и анализа тональности, когда требуется разделить текстовые данные на несколько категорий. Несмотря на то, что классическая теория мультиномиального классификатора предполагает, что на вход подается векторизованные документы по частотам слов в каждом документе, существуют работы, в которых опытным путем было показано, что tf-idf дает лучше результаты~\cite{chingmuankin}. По этой причине в работе был выбран tf-idf для мультиномиального байесовского классификатора.

    \item{\bfseries Support Vector Classifier}\\
    Метод опорных векторов состоит в поиске оптимальной разделяющей гиперплоскости, которая приводит к максимизации ширины разделяющей полосы между классами, следовательно, к более уверенной классификации. SVC работает для бинарной классификации, поэтому в случае мультиклассовой классификации приходится строить несколько алгоритмов SVC и выбирать класс с помощью голосования. В качестве подхода построения таких алгоритмов была выбрана схема one-vs-one, поскольку при использовании one-vs-rest SVC придется обучать на большой выборке данных, что вычислительно менее эффективно, так как на больших данных SVC ресурсоемкий и будет долго обучаться.


    
\end{enumerate}
\subsection{Трансформеры}
\subsubsection{Предобработка и токенизация текстов}
Для всех рассматриваемых версий BERT существуют свои специфичные токенизаторы. Перед подачей в токенизатор предложений\footnote{Под "предложением"\ будем понимать кусок текста, а не синтаксическое предложение} в каждом были убраны ссылки и имена пользователей, поскольку они не несут никакой информации об эмоциях. Несмотря на разницу в токенизаторах, отметим общие принципы: \\
\begin{itemize}
    \item {\itshape Нормализация}, т.е. приведение к нормальному виду, что обычно делается путем приведения слов к нижнему регистру, контролирования специальных символов, в том числе, удаление пробельных символов.
    \item {\itshape Токенизация}, т.е. отображение слов в токены. Для BERT - WordPiece~\cite{yonghui}, для BERTweet и RoBERTa - byte-level BPE(Byte-Pair-Encoding)~\cite{roberta}.
    \item{\itshape Добавление специальных токенов}, такик как '[CLS]', с помощью которого агрегируется информацию о предложении, используемая для задач классификации, а также '[PAD]', '[SEP]', '[EOS]'.   
\end{itemize}

\subsubsection{Модели}
В работе рассмотрены три основные модели: BERT~\cite{devlin}, RoBERTa~\cite{roberta}, BERTweet~\cite{bertweet}, а также их large-аналоги, в которых больше слоев и больше обучаемых параметров. Для решения задачи классификации брался  эмбеддинг, чья размерность 768, который соответствует позиции '[CLS]' токена в токенизированном предложении. Этот эмбеддинг проходил через dropout-слой с вероятностью обнуления элемента равной 0.3, чтобы снизить переобучение, далее через линейный слой и LogSoftMax, поскольку эта функция активации вычислительно более стабильна чем SoftMax (см. Рис. \ref{eq:proposed_model}).
\begin{figure}[h!]
	\centering{
	\includegraphics[width=160mm, height=120mm]{vis/proposed_model.pdf}
	}
    \caption{\centering Предложенная модель}
	\label{eq:proposed_model}
\end{figure}
Выбранной функцией потерь была NLLLoss(Negative Log Likelihood loss):
$$
NLLLoss(y) = - log(y)
$$
В качестве оптимизатора был выбран Adam. При обучении использовалось линейное уменьшение шага градиента и  ограничение нормы градиента с максимальной нормой, равной 1, чтобы уменьшить вероятность появления исчезающего или взрывающегося градиента.

\section{Эксперименты}
\subsection{Сравнение классических моделей машинного обучения.}
Здесь в качестве лучшей модели из основной статьи для классического метода машинного обучения брался SVM с rbf-ядром, с ней, в частности, будет приведено сравнение с другими моделями (начертание далее --- "SVM")
\subsubsection{Дизайн эксперимента.}

{\bfseries Модели для сравнения}: взвешенный метод ближайших соседей(wKNN), логистическая регрессия(LR), мультиномиальный наивный байесовский классификатор(MNB), метод опорных векторов(SVM) \\ 
\\
{\bfseries Гиперпараметры для моделей}:
\\ 
\begin{enumerate}
    \item wKNN: 
    \begin{itemize}
        \item количество соседей - 52
        \item расстояние -  косинусное
    \end{itemize}

    \item MNB: 
    \begin{itemize}
        \item сглаживание по Лапласу (Laplace smoothing)
    \end{itemize}

    \item LR:
    \begin{itemize}
        \item регуляризация - L2-регуляризация с параметром регуляризации равным 1
    \end{itemize}
    \item SVM
    \begin{itemize}
        \item ядро - ядра выбирались разные, но лучший результат(см. таблицу~\ref{table:table2}) показало $RBF(x_i, x_j) = \exp \bigl( -\gamma{\|x_i -  x_j\|^2}\bigr)$ c $\gamma = \frac{1}{n\_features * X.var()}$, где $n\_features$ - количество признаков, $X.var()$ - дисперсия, получающаяся, если вытянуть таблицу объекты-признаки в один массив и считать ее у получившихся значений.
        \item регуляризация - L2-регуляризация с параметром регуляризации равным 1
    
    \end{itemize}
    \begin{table}[h!]
        \centering
        \begin{tabular}{|c|c|c|c|}
            \hline
                Ядро & precision & recall & F1-score\\
            \hline
                 $Linear$ & 0.852 &  0.849 & 0.849\\
            \hline
                $Polynomial$ ($degree = 3$)  &  0.777 & 0.7 & 0.708\\
            \hline
                $RBF$ & \bfseries{0.873} &  \bfseries{0.871} & \bfseries{0.871} \\
            \hline
        \end{tabular}
        \caption{Различные ядра SVM на валидации.}
        \label{table:table2}
        \end{table}
\end{enumerate}

\subsubsection{Результаты и выводы}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
	\hline
		& precision & recall & F1-score\\
	\hline
		$wKNN$ & 0.701 &  0.691 & 0.693\\
	\hline
		$MNB$  &  0.778 & 0.78 & 0.776\\
	\hline
		$LR$ & 0.834 &  0.832 & 0.833 \\
	\hline
		$SVM$ & \bfseries{0.873} & \bfseries{0.871} & \bfseries{0.871} \\
	\hline
\end{tabular}
\caption{Сравнение метрик качества моделей на валидации.}
\label{table:table1}
\end{table}
Посмотрим на таблицу \ref{table:table1}, на которой изображены метрики качества моделей на валидационной выборке. Видно, что хуже всех с классификацией эмоций справляется wKNN, возможно, это связанно с тем, что для представления текстов в виде векторов необходимо использовать векторы большой размерности(размер словаря), а это неизбежно для метрических алгоритмов приводит к "проклятию размерности"{}, из-за чего wKNN становится не эффективным. Лучший результат был получен с помощью SVM с rbf-ядром благодаря способности данного метода находить нелинейные зависимости сложной структруры, поэтому ни wKNN, ни логистическая регрессия, ни мультиномиальный байесовский классификатор не смогли улучшить качество классификации по сравнению с моделью из основной статьи.

Для того, чтобы лучше понять как  модель SVM работает при детекции каждой эмоции была построена матрица ошибок на тестовой выборке, которую можно увидеть на рис. \ref{eq:conf_matr1}.
\begin{figure}[h]
	\centering{
	\includegraphics[ height=120mm]{vis/confusion_matr1.pdf}
	}
    \caption{\centering Предложенная модель}
	\label{eq:conf_matr1}
\end{figure}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
            & Joy & Anger & Fear & Sadness & Neutral \\
        \hline
            Precision & 0.92 &  0.93 & 0.83 & 0.84 & 0.70\\
        \hline
            Recall  &  0.85 & 0.88 & 0.80 & 0.87 & 0.79\\
        \hline
            F1-score & 0.89 &  0.90 & 0.82 & 0.85 & 0.74\\
        \hline
    \end{tabular}
    \caption{Precision, recall, F1-score для SVM на тестовой выборке}
    \label{table:table4}
\end{table}

Результаты показывают, что модель лучше всего работает на объектах класса "гнев"{} и хуже всего на "нейтральной"{} эмоции. Это может быть из-за того, что нейтральный класс был взят из другого распределения\footnote{Напоминание: нейтральный класс был добавлен из CrowdFlower, остальные из WASSA} (другого датасета) и таких объектов меньше чем объектов других классов, взятых из одного датасета.

\subsection{Трансформеры. Влияние эпох.}
\subsubsection{Дизайн эксперимента}
\begin{itemize}
    \item Модель: Предложенная модель с BERT
    \item Эпохи: 5, 10
    \item $lr\footnotemark = 2*10^{-5}$\footnotetext{lr - learning rate(темп обучения)} c линейным убыванием $lr$ на каждом шаге оптимизации
\end{itemize}

\subsubsection{Результаты и выводы}
Модель с BERT очень мощная и уже предобучена, поэтому, как правило, не требуется много эпох для файн-тюнинга, что и сделало ее столь популярной. Посмотрим как разное количество эпох влияет на обучение.

\begin{figure}[h]
	\centering{
	\includegraphics[width=180mm]{vis/trainhist.png}
	}
    \caption{}
	\label{eq:exp5_fig1}
\end{figure}


Видно, что модель уже на небольшом количестве эпох выходит на плато на валидационной выборке и дальнейшее увеличение количества эпох бессмысленно, поэтому далее для небольших версий  BERT использовалось 5 эпох.

\subsection{Трансформеры. Влияние размера батча.}
\subsubsection{Дизайн эксперимента}
Все параметры, не перечисленные далее, такие же как и в предыдущем эксперименте
\begin{itemize}
    \item Размеры батча: 8, 16, 32, 64
\end{itemize}
\subsubsection{Результаты и выводы}
Посмотрим как сильно влияет размер батча на предсказания:
\begin{figure}[h]
	\centering{
	\includegraphics[ height=90mm]{vis/batch_size.png}
	}
    \caption{}
	\label{eq:batch}
\end{figure}
Не сложно заметить, что влияние размера батча оказывается очень маленьким и доля верных предсказанных ответов изменяется в пределах 0.88 до 0.905 при том, что увеличение размера батча приводит к более длительному обучению модели. Несмотря на то, что 16 объектов в батче дает неплохие результаты и по времени, и по метрике, в дальнейшем в экспериментах будет использоваться размер батча равный 32. 

\subsection{Трансформеры. Темп обучения.}
\subsubsection{Дизайн эксперимента}
В качестве базовой модели по-прежнему бралась предложенная модель с BERT, 5 эпох обучения и размер батча равный 32. Для lr бралось 6 значений по логарифмической сетке от $10^{-6}$ до $10^{-3}$ и дальше на каждом шаге оптимизации lr линейно уменьшался. 
\subsubsection{Результаты и выводы}
\begin{figure}[h]
	
	\centering{
	\includegraphics[height=90mm]{vis/lrs.png}
	}
    \caption{}
	\label{eq:lrs}
\end{figure}
Видно,  что из-за слишком маленького количества эпох при малых значениях модель не дообучается ($lr = 10^{-6},\ lr = 3.126 * 10^{-6},\ lr = 10^{-4}$), а при слишком больших ($lr = 10^{-3}$) вообще не способна обучаться, поскольку градиент становится большим и, видимо, перескакивает точку минимума. Оптимально брать значения в диапазоне от $2*10^{-5}$ до $10^{-5}$. Для версий BERT с малым количеством параметров в дальнейшем будет браться $lr=2*10^{-5}$.

\subsection{Сравнение различных версий BERT}
Здесь в качестве лучшей модели из основной статьи для бертов брался BERTweet, с ним, в частности, будет приведено сравнение с другими версиями берта (начертание далее --- "BERTweet")
\subsubsection{Дизайн эксперимента}
В качестве моделей рассматривались BERT, BERT-large, RoBERTa, RoBERTa-large, BERTweet, BERTweet-large. Для моделей с маленьким числом обучаемых параметров(BERT, RoBERTa, BERTweet) хорошо сработали те параметры, что рассматривались в предыдущих экспериментах. Для моделей с суффиксом "large"{}, у которых больше параметров, пришлось подбирать новые гиперпараметры, потому что модели не дообучались. Эти гиперпараметры оказались одинаковыми для каждой large-версии:
\begin{itemize}
    \item Количество эпох: 7
    \item Размер батча: 32
    \item Темп обучения(lr): $5 * 10^{-6}$ c линейным убыванием на каждом шаге оптимизации.
\end{itemize}
\subsubsection{Результаты и выводы}
После обучения моделей на датасете из твитов, описанном в начале работы, были построены графики доли правильных ответов от эпох(см.~Рис.~\ref{eq:models_anal})
\begin{figure}[h]
    \centering
    \makebox[\linewidth]{%
      \setlength\fboxsep{0pt}%<- optional for padding
      \colorbox{white}{%
        \includegraphics[width=1.25\linewidth]{vis/models_anal.png}}}
    \caption{}
    \label{eq:models_anal}
\end{figure}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
            & precision & recall & F1-score\\
        \hline
            $RoBERTa_{base}$ & 0.8827 &  0.8834 & 0.8829\\
        \hline
            $RoBERTa_{large}$  &  0.8861 & 0.8853 & 0.8855\\
        \hline
            $BERT_{base}$ & 0.8968 &  0.8933 & 0.8940 \\
        \hline
            $BERT_{large}$& 0.8923 & 0.8920 & 0.8921 \\
        \hline
            $BERTweet_{base}$ & \bfseries{0.9004} &  \bfseries{0.8987} & \bfseries{0.8992} \\
        \hline
            $BERTweet_{large}$ & 0.8954 &  0.8933 & 0.8939 \\    
        \hline
    \end{tabular}
    \caption{Сравнение метрик качества моделей на валидации.}
    \label{table:table3}
    \end{table}

На левом графике видно, что все модели, в целом, имеют небольшое переобучение, а значит являются потенциально достаточно надежными моделями, то есть на других данных из твитов будут давать примерно такой же результат как и на валидации.

Несмотря на то, что большее количество параметров может приводить к более высоким результатам по метрикам качества, очевидно из правого графика, что единственная модель, которой удалось побить качество своей маленькой версии является $RoBERTa_{large}$.

Из всех моделей самое высокое значение $accuracy = 0.8987$ было получено у модели $BERTweet$ из статьи, что объясняется двумя факторами. Во-первых, $BERTweet$ была предобучена по той же схеме, что и $RoBERTa$, а она, в свою очередь, является улучшенной версией $BERT$, которая превзошла $BERT$ во многих задачах. Во-вторых, $BERTweet$ предобучена на 850M твитов и как раз ориентирована на наш датасет.

Таким образом, предположение о том, что large-версии бертов смогут заметно улушить качество исходной модели из статьи, оказалось в данной задаче безуспешным.

Также была построена матрица ошибок(см. Рис. \ref{eq:conf_matr2}). На ней видно, что accuracy модели в сравнении с SVM значительно улучшилось на таких классах как: 'радость' (на $\approx\! 0.06$), 'страх' (на $\approx\! 0.03$), 'грусть' (на $\approx\! 0.04$) и 'нейтральная' эмоция (на $\approx\! 0.11$). Тем не менее есть одна эмоция, а именно, 'гнев', которая лучше предсказывается SVM. (на $\approx\! 0.035$). Заметим еще, что в случае использования предложенной модели на основе BERTweet, получилось добиться accuracy по всем классам на $\approx\! 0.04$ лучше чем у SVM.
\begin{figure}[h]
	
	\centering{
	\includegraphics[height=90mm]{vis/conf_matr2.png}
	}
    \caption{\centering Матрица ошибок для BERTweet на тестовой выборке}
	\label{eq:conf_matr2}
\end{figure} 

\section{Заключение}
В данной работе были проанализированы и сравнены несколько моделей для детекции эмоций. Проведены эксперименты на данных, состоящих из твитов, рассмотрены некоторые элементы предобработки таких данных. Из исследования можно сделать следующие выводы:
\begin{itemize}
    \item wKNN, логистическая регрессия, модели SVM с линейным, полиномиальным ядрами, мультиномиальный байесовский классификатор хуже сработали чем модель SVM из исходной статьи.
    \item SVM с rbf-ядром и TF-IDF при правильной обработке данных дает неплохие результаты ($accuracy = 0.8387$) при том, что обучение занимает меньше времени чем у трансформеров.
    \item Модель с BERTweet позволяет получить наилучшее качество для данных из твитов с $accuracy = 0.88$.
    \item BERT версии с большим количеством параметров, так называемые, large-модели, не показали в данной задаче  заметного улучшения в сравнении с base-версиями (меньшим числом параметров) и не смогли превзойти BERTweet из исходной статьи.
    \item BERT версии не требуют большого количества эпох для обучения, что связанно с предобученностью моделей
    \item Размер батча слабо влиял на качество, важно было подобрать правильный темп обучения. В итоге для малых версий бертов подошел $lr = 2 * 10^{-5}$, а для больших $lr = 5 * 10^{-6}$
\end{itemize}

В дальнейшем исследовании, можно попробовать использовать более умные эмбеддинги для классических моделей машинного обучения, которые не просто собирают статистики, а могут запоминать контекст слов (GloVe, FastText, ELMO), также было бы интересно проверить гипотезу о возможном улучшении классификации в случае добавления в эмбеддинги информации о наиболее часто встречаемой эмоции для этого слова, собранной из словарей, в которых каждому слову сопоставлена эмоция. 
\newpage
\bibliographystyle{plain}
\bibliography{references}

\end{document}