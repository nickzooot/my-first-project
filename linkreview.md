# Обзор статей
| # | Название | Ссылка | О чём | Релевантность |
| - | -------- | ------ | ----- | ------ | 
| 1 |Exploration and chronicling civilian sentiment and response to terrorism events via twitter | [link](https://www.researchgate.net/publication/220198823_A_microblogging-based_approach_to_terrorism_informatics_Exploration_and_chronicling_civilian_sentiment_and_response_to_terrorism_events_via_Twitter) | Распознаются агрессивные эмоции, что помогает выявить потенциальных преступников или террористов| Средне релевантна|
| 2 | Examining Accumulated Emotional Traits in Suicide Blogs With an Emotion Topic Model| [link](https://www.researchgate.net/publication/280388209_Examining_Accumulated_Emotional_Traits_in_Suicide_Blogs_With_an_Emotion_Topic_Model) | Мониторинг эмоций в реальном времени на основе данных социальных сетей может помочь в профилактике самоубийст| Cредне релевантна|
| 3 | Current State of Text Sentiment Analysis from Opinion to Emotion Mining | [link](https://webdocs.cs.ualberta.ca/~zaiane/postscript/survey-SentimentAnalysis.pdf) | Про различия "распознавания эмоций" и "анализ тональности" | Средне релевантна |
| 4 | WASSA-2017 Shared Task on Emotion Intensit | [link](https://arxiv.org/pdf/1708.03700.pdf) | Датасет из твитов для детекции эмоций | Релевантна|
| 5 | Emotion detection from tweets using a bert and svm ensemble model | [link](https://www.researchgate.net/publication/362591831_Emotion_Detection_From_Tweets_Using_a_BERT_and_SVM_Ensemble_Model) | Решают задачу с помощью ансамбля SVM и BERT | Релевантна|
| 6 | Harnessing Twitter 'Big Data' for Automatic Emotion Identification | [link](https://www.researchgate.net/publication/258762213_Harnessing_Twitter_'Big_Data'_for_Automatic_Emotion_Identification) | Применили логистическую регрессию и Naiıve Bayes, чтобы исследовать эффективность различных признаков, таких как n-граммы, лексикон эмоций и информация о части речи, для задачи идентификации эмоций. Наибольшая достигнутая точность составила 0,6557 | Релевантна|
| 7 | Emotional Tweets | [link](https://aclanthology.org/S12-1033.pdf) | Cоздал корпус твитов, маркированных эмоциями, используя хэштеги. Он применил бинарные SVM, по одному для каждой из шести основных эмоций Экмана, и использовал наличие или отсутствие униграмм и биграмм в качестве бинарных признаков. Бинарные классификаторы смогли предсказать эмоции со сбалансированным F1-score 0,499 | Релевантна |
| 8 | An argument for basic emotions, cognition and emotion.  | [link](http://gruberpeplab.com/3131/1.2_Ekman1992_Basic%20Emotions.pdf) |Ввели шесть основных эмоций| Релевантна |
| 9 | Influence of weak labels for emotion recognition of tweet| [link](https://www.researchgate.net/publication/275517133_Influence_of_Weak_Labels_for_Emotion_Recognition_of_Tweets) | Использовали слабые метки и методы: стохастический градиентный спуск, SVM, Naıve Bayes, Nearest Centroid и Ridge. Результаты показали снижение F1-score на 9,25% при использовании слабых меток. | Релевантна|
| 10 | Understanding Emotions in Text Using Deep Learning and Big Data | [link](https://www.researchgate.net/publication/329834468_Understanding_Emotions_in_Text_Using_Deep_Learning_and_Big_Data) | Используется LSTM для распознавания эмоций | Средне релевантна|
| 11 | Deep learning for affective computing: Text-based emotion recognition in decision support | [link](https://arxiv.org/pdf/1803.06397.pdf) | Используется LSTM для распознавания эмоций | Средне релевантна |
| 12 | Long short-term memory.| [link](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory) | Про LSTM и ограниченную способность улавливать долгосрочные зависимости в тексте| Не релевантна |
| 13 | Bert: pre-training of deep bidirectional transformers for language understanding | [link](https://arxiv.org/pdf/1810.04805.pdf) | Про BERT| Релевантна |
| 14 | Improving Language Understanding by Generative Pre-Training| [link](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) | Про GPT от OpenAI | Не релевантна |
| 15 | Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context | [link](https://arxiv.org/pdf/1901.02860.pdf) | Про Transormer-XL и его возможности улавливать более длинные зависимости чем RNN| Релевантна |
| 16 | Transfer learning for sentiment analysis using bert based supervised fine-tuning | [link](https://www.researchgate.net/publication/360941776_Transfer_Learning_for_Sentiment_Analysis_Using_BERT_Based_Supervised_Fine-Tuning) | Использует BERT в качестве слоя эмбеддинга, после которого выводы передаются через слои CNN и BiLSTM для анализа настроений на бенгальском языке| Релевантна|
